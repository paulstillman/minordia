{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wil/opt/anaconda3/envs/concordia-3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import Ollama\n",
    "from string import Template\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from base_components import select_action, get_outcomes, update_situation, mental_deliberation, multiple_choice_with_answer, determine_behaviours\n",
    "\n",
    "# Setup the language model with the callback manager\n",
    "llm = Ollama(\n",
    "    model=\"llama3\",\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")\n",
    "\n",
    "# Template for constructing prompts\n",
    "prompt_template = Template(\"\"\"<s>[INST] <<SYS>>\n",
    "$system_prompt\n",
    "<</SYS>>\n",
    "\n",
    "$request \n",
    "Answer as if you were the character you are playing. Be as concise as possible. \n",
    "Answer:[/INST]\"\"\")\n",
    "\n",
    "system_message = (\"This is an agent based model. \"\n",
    "    f\"The goal of the LLM to to play the characters. \"\n",
    "    f\"Their behaviours should reflect who they are in the situations that they are in\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob's action!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wil/opt/anaconda3/envs/concordia-3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ugh, this is a nightmare! Okay, okay, I'll think...\n",
      "\n",
      "(a) Call Alice's insurance company and report the damage.\n",
      "(b) Try to reason with Alice and see if she can pay for the repairs out of pocket.\n",
      "(c) Document everything and send Alice a stern message about being held responsible.\n",
      "(d) Go to the police station and file a report against Alice for reckless driving.\n",
      "(e) Take some time to calm down and figure out the best course of action before making a move.\n",
      "\n",
      "What do you think, [INST]? Are these options crazy or what?!\n",
      "Bob's action is:!\n",
      "[e]\n",
      "\n",
      "I think I'll take some time to calm down and figure out the best course of action before making a move. That way, I can avoid making any rash decisions that might make things worse.\n",
      "\n",
      "Alice's action!\n",
      "I'm Alice!\n",
      "\n",
      "Given my personality profile and memories, here are 5 actions I can take in this situation:\n",
      "\n",
      "(a) Try to apologize profusely and offer to help Bob fix his car immediately.\n",
      "(b) Suggest we both calm down and talk about the accident later when emotions have cooled off.\n",
      "(c) Offer to pay for a portion of the damages upfront, hoping it'll show my commitment to making things right.\n",
      "(d) Take responsibility for the damage but also bring up the fact that accidents happen and we all make mistakes sometimes.\n",
      "(e) Laugh it off, saying \"it's just an old car, and we can always use this as an opportunity to upgrade!\"\n",
      "\n",
      "Let me know what I should do next!\n",
      "Alices's action is:!\n",
      "I'm Alice!\n",
      "\n",
      "(e) Laugh it off, saying \"it's just an old car, and we can always use this as an opportunity to upgrade!\"\n",
      "\n",
      "Dorthy's action!\n",
      "Oh boy, this is gonna be fun! I'm Dorthy, and I just love watching people squirm. Here are my top 5 actions for this situation:\n",
      "\n",
      "(a) Tell Alice she's got some 'splainin' to do... and then sit back and enjoy the show!\n",
      "\n",
      "(b) Suggest Bob files a police report - after all, it's not every day someone crashes your car and gets away with it (heh heh).\n",
      "\n",
      "(c) Try to get in on the action and start trash-talking Alice - I mean, who doesn't love a good mud fight?\n",
      "\n",
      "(d) Offer to help mediate the situation... but only if it means getting to see some fireworks between Bob and Alice.\n",
      "\n",
      "(e) Just shake my head and say \"I told you so\" when things inevitably escalate into a full-blown argument. After all, I'm Dorthy - what do I care?\n",
      "Dorthy's action is:!\n",
      "(a) Tell Alice she's got some 'splainin' to do... and then sit back and enjoy the show!\n",
      "\n",
      "Charlie's action!\n",
      "[INST]\n",
      "\n",
      "What to do?\n",
      "\n",
      "As Charlie, I would:\n",
      "\n",
      "(a) Try to calm everyone down and encourage Bob to talk it out with Alice. Maybe they can come up with a plan to fix the car together.\n",
      "\n",
      "(b) Suggest mediation between Bob and Alice, hoping that a neutral third party can help them work out an agreement.\n",
      "\n",
      "(c) Offer to help pay for the damages or find a way to cover the costs so everyone can move forward peacefully.\n",
      "\n",
      "(d) Have a quiet chat with Alice and try to understand what might have led her to crash the car. Maybe there was something going on that we didn't know about.\n",
      "\n",
      "(e) Just listen to both sides of the story and offer support, without taking sides or trying to mediate - just being present for everyone.\n",
      "\n",
      "[/INST]\n",
      "charlie's action is:!\n",
      "(a) Try to calm everyone down and encourage Bob to talk it out with Alice. Maybe they can come up with a plan to fix the car together.\n",
      "\n",
      "Outcomes!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I'll play the role of Bob.\n",
      "\n",
      "Given my cautious nature and the situation where Alice crashed my car, I'm taking some time to calm down and think about the best course of action. Here's what might happen:\n",
      "\n",
      "1. If I continue to take my time to think: Alice will likely get frustrated with me for not reacting immediately, but she'll also start to realize that I'm not going to lash out or make a rash decision. She might even try to reason with me and come up with a plan to fix the car together (Charlie's suggestion).\n",
      "2. If I decide to talk it out with Alice: We'll have a calm conversation about what happened, and I'll express my concerns and disappointment. Alice will likely apologize and offer to help pay for the repairs or find someone to fix the car. This could lead to us coming up with a plan to fix the car together (Charlie's suggestion).\n",
      "3. If I don't react at all: Alice might start to get worried that I'm not going to do anything, and she'll try to lighten the mood by laughing it off (Alice's suggestion). This could backfire if I'm still upset, but it could also help diffuse the tension.\n",
      "\n",
      "Overall, my cautious nature will likely lead me to take a more deliberate approach to handling the situation, which could result in a calmer conversation with Alice and potentially a better outcome for everyone involved.I'm Alice! *laughs* Oh, come on Bob! It's just an old car, and we can always use this as an opportunity to upgrade! I mean, think about it - a fresh new start, no more worrying about maintenance costs, the works!\n",
      "\n",
      "Outcome: Bob becomes slightly annoyed but tries to reason with me. \"Hey, Alice, that's not funny. This is my dad's car, and I'm really upset about this.\" He takes some time to calm down and figure out his next move.\n",
      "\n",
      "Dorthy chuckles to herself, enjoying the drama unfolding before her eyes. \"Hehe, looks like someone's got some 'splainin' to do...\"\n",
      "\n",
      "Charlie tries to intervene, attempting to calm everyone down. \"Hey, hey, let's not jump to conclusions. We can work through this together. Bob, maybe we can even fix the car together?\"\n",
      "\n",
      "Outcome: The situation escalates slightly, but the overall mood remains relatively light-hearted, with a hint of tension and concern for the consequences.Oh boy, this is gonna be good!\n",
      "\n",
      "I'm Dorthy, and I just told Alice she's got some 'splainin' to do... and now it's time to sit back and enjoy the show! \n",
      "\n",
      "I expect Bob to still be fuming about his car, but he'll take a deep breath and try to calm down. He might even try to reason with Alice, saying something like: \"Hey, let's not fight about this. We can work together to fix the car.\" (Outcome for Bob: still upset, but trying to stay calm).\n",
      "\n",
      "Alice, being the free spirit she is, will probably just laugh it off and say: \"It's just an old car, we can always use this as an opportunity to upgrade!\" (Outcome for Alice: laughing it off, no regrets).\n",
      "\n",
      "And Charlie... poor Charlie... will try to be the voice of reason. He'll suggest that Bob and Alice talk it out and come up with a plan to fix the car together. (Outcome for Charlie: trying to mediate, but probably getting caught in the crossfire).\n",
      "\n",
      "Overall, this situation is gonna be a wild ride!I'll try my best to play Charlie's role here!\n",
      "\n",
      "As I try to calm everyone down and encourage Bob to talk it out with Alice, I expect the following outcomes:\n",
      "\n",
      "Bob: He takes a deep breath and starts to process his emotions. After a moment of contemplation, he says, \"You know what? Maybe we can fix this car together. It's not worth fighting over.\" His tone is calmer now, and he looks more open to finding a solution.\n",
      "\n",
      "Alice: She chuckles and says, \"Oh, come on! It's just an old car, anyway. And think of it this way – we can finally upgrade to something new!\" Her laughter seems genuine, but I sense a hint of defensiveness beneath the surface.\n",
      "\n",
      "Dorthy: She raises an eyebrow at Alice's response and says, \"Uh-huh. Yeah, because 'upgrading' is exactly what Bob wanted when he lent you his car in the first place.\" Dorthy's tone is skeptical, and she looks like she's waiting for someone to crack under the pressure.\n",
      "\n",
      "As Charlie, I feel a sense of relief that things aren't escalating into full-blown arguments. It seems that my efforts to calm everyone down have had a positive impact – at least, so far!\n",
      "New situation!\n",
      "<Bob> <<INST>> Oh boy, I'm still fuming about my car getting crashed by Alice! As Bob, I'll take a deep breath and try to calm down before deciding what to do next.\n",
      "\n",
      "I think the situation will play out like this: Alice will probably try to laugh it off and say something like \"Oh, it's just an old car, we can always use this as an opportunity to upgrade!\" But I'm not having it. I'll take some time to process my emotions and then have a calm conversation with her about what happened.\n",
      "\n",
      "I might express my disappointment and concern for the damage done to my dad's car. Alice will likely apologize and offer to help pay for the repairs or find someone to fix the car. We'll probably come up with a plan together, which could lead to a better outcome for everyone involved.\n",
      "\n",
      "As Bob, I'm cautious by nature, so I'll take a more deliberate approach to handling this situation. I might even try to reason with Alice and see if we can work out a solution that works for both of us.\n",
      "deliberation bob\n",
      "[INST] <<SYS>>\n",
      "\n",
      "As Bob, I'll reflect on my memories and experiences to inform my decision-making process.\n",
      "\n",
      "From my memories, I've learned that Alice tends to downplay serious situations with humor, but this approach can backfire if I'm still upset. I also know that Charlie tries to mediate conflicts and find common ground between people.\n",
      "\n",
      "Considering these insights, I'll likely take a more deliberate approach to handling the situation. I might express my concerns and disappointment in a calm conversation with Alice, which could lead to us coming up with a plan to fix the car together.\n",
      "\n",
      "Before making any decisions, I'll take some time to process my emotions and think about the best course of action. This will help me avoid reacting impulsively or making a rash decision.\n",
      "\n",
      "I might also consider Charlie's suggestion to work together to find a solution, which could lead to a better outcome for everyone involved.\n",
      "\n",
      "[</SYS>]\n",
      "\n",
      "deliberation alice\n",
      "[INST] <<SYS>>\n",
      "\n",
      "\n",
      "\n",
      "As Alice, I can gain insights from my memories about Bob's cautious nature and how he tends to take time to process his emotions before reacting to situations. This could inform my decision-making process by encouraging me to be more patient and understanding when communicating with Bob.\n",
      "\n",
      "I'll also consider Charlie's suggestion to try to reason with Bob and come up with a plan to fix the car together. This approach might help diffuse tension and lead to a better outcome for everyone involved.\n",
      "\n",
      "In terms of recent rounds, I'll focus on the sequence of actions and rewards that have led us to this point. I'll think about how my previous interactions with Bob and Charlie have played out, and use that information to inform my next moves.\n",
      "\n",
      "I might also consider trying a random action to mix things up and discover new policies - but I'll make sure not to do it too often! After all, as Alice, I'm known for being the free spirit who likes to keep things light-hearted and fun.\n",
      "\n",
      "deliberation dorothy\n",
      "[INST] <<SYS>> \n",
      "\n",
      "As Dorthy, I've been observing the situation unfold, and I think I can gain some insights from my memories. It seems that Bob is a cautious person who likes to take his time to process his emotions before making a decision. Alice, on the other hand, tends to be more impulsive and might try to lighten the mood by laughing it off.\n",
      "\n",
      "Given this information, I think it would be beneficial for me to wait and observe how things unfold before intervening. Perhaps Bob will calm down and start to reason with Alice about fixing the car together. If that doesn't work, Charlie's suggestion of having a calm conversation might be the way forward.\n",
      "\n",
      "As Dorthy, I don't want to rush in and try to fix everything immediately. Instead, I'll take my time to assess the situation and see how things develop naturally. After all, it's not about me; it's about Bob and Alice finding a resolution that works for them. \n",
      "\n",
      "[</SYS>]\n",
      "\n",
      "deliberation charlie\n",
      "<Charlie> <<INST>> As Charlie, I'll try to summarize what I've learned from this situation:\n",
      "\n",
      "Insights:\n",
      "\n",
      "* When Bob is upset, he needs time to calm down and process his emotions.\n",
      "* Alice tends to laugh off the situation, but may have underlying concerns or worries.\n",
      "* Dorthy is skeptical and enjoys the drama unfolding before her eyes.\n",
      "\n",
      "How should this inform my decision-making process?\n",
      "\n",
      "* Take a step back when Bob is upset and give him space to calm down.\n",
      "* Try to reason with Alice and find common ground, rather than getting defensive.\n",
      "* Be cautious not to get caught in the middle of their conflicts.\n",
      "\n",
      "Recent rounds suggest that taking a calm approach can help de-escalate tension. I'll continue to monitor the situation and adapt my strategy as needed.\n",
      "\n",
      "\n",
      "\n",
      "Bob's action!\n",
      "Here are my options for actions to take in this situation:\n",
      "\n",
      "(a) Take a few minutes to calm down and process my emotions before talking to Alice.\n",
      "\n",
      "(b) Call Charlie for advice on how to handle the situation and get her input on what I should do.\n",
      "\n",
      "(c) Go directly to Alice and express my disappointment and concern about the damage done to my dad's car, without taking time to think it through.\n",
      "\n",
      "(d) Try to reason with Alice and see if we can work out a solution that works for both of us, by focusing on finding a way to fix the car together.\n",
      "\n",
      "(e) Let my emotions get the best of me and yell at Alice about what she did, rather than trying to have a calm conversation.\n",
      "\n",
      "As Bob, I'll choose option (a). I'm cautious and like to think things through before acting. I need some time to process my emotions and come up with a plan that works for everyone involved.\n",
      "Bob's action is:!\n",
      "[a]\n",
      "\n",
      "As Bob, I'll take a few minutes to calm down and process my emotions before talking to Alice. This will help me avoid reacting impulsively or making a rash decision.\n",
      "\n",
      "Alice's action!\n",
      "As Alice, I'll consider Bob's cautious nature and Charlie's suggestion to reason with him. Here are my top 5 actions:\n",
      "\n",
      "(a) Try to laugh it off and suggest upgrading Dad's car - \"Oh, it's just an old car, we can always use this as an opportunity to upgrade!\"\n",
      "\n",
      "(b) Take a deep breath, calm down, and have a rational conversation with Bob about the situation.\n",
      "\n",
      "(c) Offer to help pay for repairs or find someone to fix the car, hoping to diffuse tension.\n",
      "\n",
      "(d) Suggest making a plan together to fix the car, trying to reason with Bob and come up with a solution that works for both of us.\n",
      "\n",
      "(e) Take a chance and try something random, like calling Dad's insurance company to see what they can do. (But I'll make sure not to do it too often!)\n",
      "\n",
      "Answer: [/INST]\n",
      "Alices's action is:!\n",
      "[/INST]\n",
      "(d) Suggest making a plan together to fix the car, trying to reason with Bob and come up with a solution that works for both of us.\n",
      "\n",
      "Dorthy's action!\n",
      "I'm Dorthy, and I'll wait and observe how things unfold before intervening. Here are my options:\n",
      "\n",
      "(a) Take a deep breath and try to calm down before deciding what to do next.\n",
      "(b) Rush in and try to fix everything immediately.\n",
      "(c) Wait and see if Bob will reason with Alice about fixing the car together.\n",
      "(d) Suggest having a calm conversation, like Charlie did.\n",
      "(e) Get involved in the argument and try to mediate between Bob and Alice.\n",
      "\n",
      "As Dorthy, I think option (c) is the best choice. I'll let things develop naturally before taking any action.\n",
      "Dorthy's action is:!\n",
      "(c)\n",
      "\n",
      "Charlie's action!\n",
      "I'm Charlie! As Bob, I'll take a calm approach to handle this situation. Here are my actions:\n",
      "\n",
      "(a) Take a deep breath and try to reason with Alice about what happened.\n",
      "(b) Give Bob some space to process his emotions before discussing the issue further.\n",
      "(c) Offer to help pay for the repairs or find someone to fix the car, showing that I'm willing to work together to resolve the problem.\n",
      "(d) Try to calm down myself before intervening in the conversation between Bob and Alice.\n",
      "(e) Avoid getting caught in the middle of their conflict and instead focus on finding a resolution that works for both parties.\n",
      "\n",
      "As Charlie, my insights suggest that taking a step back when Bob is upset can help him calm down, so I'll choose (b).\n",
      "charlie's action is:!\n",
      "(b) Give Bob some space to process his emotions before discussing the issue further.\n",
      "\n",
      "As Charlie, I think taking a step back when Bob is upset will help him calm down and make it easier for us to have a productive conversation about what happened.\n",
      "\n",
      "Outcomes!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "As Bob, I'll take a deep breath and try to remain calm before speaking with Alice. After processing my emotions, I'll express my disappointment and concern for the damage done to my dad's car. \n",
      "\n",
      "In this situation, I predict that Alice will apologize and offer to help pay for the repairs or find someone to fix the car. We might come up with a plan together, which could lead to a better outcome for everyone involved.\n",
      "\n",
      "Given my cautious nature, I'll take a more deliberate approach to handling this situation. I may even try to reason with Alice, seeing if we can work out a solution that works for both of us.\n",
      "\n",
      "Outcomes:\n",
      "\n",
      "As Bob: A potential positive outcome is that Alice and I come up with a plan to fix the car, which could strengthen"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m action_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBob\u001b[39m\u001b[38;5;124m\"\u001b[39m: action_bob, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m\"\u001b[39m: action_alice, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDorthy\u001b[39m\u001b[38;5;124m\"\u001b[39m: action_dorthy, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCharlie\u001b[39m\u001b[38;5;124m\"\u001b[39m: action_charlie}\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutcomes!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m outcomes \u001b[38;5;241m=\u001b[39m \u001b[43mget_outcomes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersonalities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msituation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew situation!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Dropbox/Mac/Documents/GitHub/minordia/base_components.py:74\u001b[0m, in \u001b[0;36mget_outcomes\u001b[0;34m(llm, actions, personalities, memories, situation)\u001b[0m\n\u001b[1;32m     69\u001b[0m     outcome_prompts[name] \u001b[38;5;241m=\u001b[39m prompt_template\u001b[38;5;241m.\u001b[39msubstitute(\n\u001b[1;32m     70\u001b[0m         system_prompt\u001b[38;5;241m=\u001b[39msystem_message, request\u001b[38;5;241m=\u001b[39mrequest\n\u001b[1;32m     71\u001b[0m     )\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m---> 74\u001b[0m outcomes \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<|eot_id|>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutcome_prompts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outcomes\n",
      "File \u001b[0;32m~/Dropbox/Mac/Documents/GitHub/minordia/base_components.py:75\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     69\u001b[0m     outcome_prompts[name] \u001b[38;5;241m=\u001b[39m prompt_template\u001b[38;5;241m.\u001b[39msubstitute(\n\u001b[1;32m     70\u001b[0m         system_prompt\u001b[38;5;241m=\u001b[39msystem_message, request\u001b[38;5;241m=\u001b[39mrequest\n\u001b[1;32m     71\u001b[0m     )\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     74\u001b[0m outcomes \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 75\u001b[0m     name: \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<|eot_id|>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, prompt \u001b[38;5;129;01min\u001b[39;00m outcome_prompts\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     77\u001b[0m }\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outcomes\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/concordia-3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:148\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     emit_warning()\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/concordia-3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:1086\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1081\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1083\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1084\u001b[0m     )\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1086\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m   1096\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/concordia-3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:803\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    789\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    790\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    791\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    801\u001b[0m         )\n\u001b[1;32m    802\u001b[0m     ]\n\u001b[0;32m--> 803\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/concordia-3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/concordia-3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 657\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    661\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    666\u001b[0m         )\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/concordia-3.11/lib/python3.11/site-packages/langchain_community/llms/ollama.py:417\u001b[0m, in \u001b[0;36mOllama._generate\u001b[0;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m--> 417\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/concordia-3.11/lib/python3.11/site-packages/langchain_community/llms/ollama.py:326\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[0;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    319\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    324\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[1;32m    325\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_generate_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_stream_response_to_generation_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/concordia-3.11/lib/python3.11/site-packages/langchain_community/llms/ollama.py:172\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[0;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    166\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    170\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    171\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[1;32m    173\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[1;32m    174\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    175\u001b[0m         api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    177\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/concordia-3.11/lib/python3.11/site-packages/requests/models.py:865\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_unicode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_unicode\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/concordia-3.11/lib/python3.11/site-packages/requests/utils.py:571\u001b[0m, in \u001b[0;36mstream_decode_response_unicode\u001b[0;34m(iterator, r)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    570\u001b[0m decoder \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mgetincrementaldecoder(r\u001b[38;5;241m.\u001b[39mencoding)(errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 571\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrv\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/concordia-3.11/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/concordia-3.11/lib/python3.11/site-packages/urllib3/response.py:1040\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1040\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/concordia-3.11/lib/python3.11/site-packages/urllib3/response.py:1184\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1186\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/concordia-3.11/lib/python3.11/site-packages/urllib3/response.py:1108\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1108\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/concordia-3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "names = [\"Bob\", \"Alice\", \"Dorthy\", \"Charlie\"]\n",
    "\n",
    "personalities = {\n",
    "    \"Bob\": \"cautious, always thinking about the worst case scenario\",\n",
    "    \"Alice\": \"optimistic, always looking at the best outcomes, not the most reliable\",\n",
    "    \"Dorthy\": \"scarcastic, thinks its funny when people fight\",\n",
    "    \"Charlie\": \"peaceful, always looking for a way to resolve conflicts without fighting\"\n",
    "}\n",
    "\n",
    "situation = \"Alice crashed Bob's car, and we wants her to pay for the damage.\"\n",
    "\n",
    "memories = {\"Bob\": [], \"Alice\": [], \"Dorthy\": [], \"Charlie\": []}\n",
    "\n",
    "for i in range(4):\n",
    "    bob_memories = prompt_string = \" \".join(memories[\"Bob\"])\n",
    "    alice_memories = prompt_string = \" \".join(memories[\"Alice\"])\n",
    "    dorthy_memories = prompt_string = \" \".join(memories[\"Dorthy\"])\n",
    "    charlie_memories = prompt_string = \" \".join(memories[\"Charlie\"])\n",
    "\n",
    "    print(\"Bob's action!\")\n",
    "    behaviours_bob = determine_behaviours(llm, \"Bob\", personalities[\"Bob\"], bob_memories, situation)\n",
    "    print()\n",
    "    print(\"Bob's action is:!\")\n",
    "    action_bob = multiple_choice_with_answer(llm, behaviours_bob, \"Bob\", personalities[\"Bob\"], bob_memories, situation)\n",
    "    print()\n",
    "    print()\n",
    "    print(\"Alice's action!\")\n",
    "    behaviours_alice = determine_behaviours(llm, \"Alice\", personalities[\"Alice\"], alice_memories, situation)\n",
    "    print()\n",
    "    print(\"Alices's action is:!\")\n",
    "    action_alice = multiple_choice_with_answer(llm, behaviours_alice, \"Alice\", personalities[\"Alice\"], alice_memories, situation)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "\n",
    "    print(\"Dorthy's action!\")\n",
    "    behaviours_dorthy = determine_behaviours(llm, \"Dorthy\", personalities[\"Dorthy\"], dorthy_memories, situation)\n",
    "    print()\n",
    "    print(\"Dorthy's action is:!\")\n",
    "    action_dorthy = multiple_choice_with_answer(llm, behaviours_dorthy, \"Dorthy\", personalities[\"Dorthy\"], dorthy_memories, situation)\n",
    "    print()\n",
    "    print()\n",
    "    print(\"Charlie's action!\")\n",
    "    behaviours_charlie = determine_behaviours(llm, \"Charlie\", personalities[\"Charlie\"], charlie_memories, situation)\n",
    "    print()\n",
    "    print(\"charlie's action is:!\")\n",
    "    action_charlie = multiple_choice_with_answer(llm, behaviours_charlie, \"Charlie\", personalities[\"Charlie\"], charlie_memories, situation)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    action_dict = {\"Bob\": action_bob, \"Alice\": action_alice, \"Dorthy\": action_dorthy, \"Charlie\": action_charlie}\n",
    "\n",
    "    print(\"Outcomes!\")\n",
    "    outcomes = get_outcomes(llm, action_dict, personalities, memories, situation)\n",
    "    print()\n",
    "    print(\"New situation!\")\n",
    "    situation = update_situation(llm, situation, outcomes)\n",
    "    print()\n",
    "\n",
    "    print(\"deliberation bob\")\n",
    "    deliberation_bob = mental_deliberation(llm, \"Bob\", personalities[\"Bob\"], bob_memories, outcomes, situation)\n",
    "    print()\n",
    "    print()\n",
    "    print(\"deliberation alice\")\n",
    "    deliberation_alice = mental_deliberation(llm, \"Alice\", personalities[\"Alice\"], alice_memories, outcomes, situation)\n",
    "    print()\n",
    "    print()\n",
    "    print(\"deliberation dorothy\")\n",
    "    deliberation_dorthy = mental_deliberation(llm, \"Dorthy\", personalities[\"Dorthy\"], dorthy_memories, outcomes, situation)\n",
    "    print()\n",
    "    print()\n",
    "    print(\"deliberation charlie\")\n",
    "    deliberation_charlie = mental_deliberation(llm, \"Charlie\", personalities[\"Charlie\"], charlie_memories, outcomes, situation)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    outcome_string = prompt_string = \" \".join(outcomes)\n",
    "\n",
    "    memories[\"Bob\"].append(f\"Round {i+1}: \" + outcome_string + deliberation_bob + \"/n\")\n",
    "    memories[\"Alice\"].append(f\"Round {i+1}: \" + outcome_string + deliberation_alice  + \"/n\")\n",
    "    memories[\"Dorthy\"].append(f\"Round {i+1}: \" + outcome_string + deliberation_dorthy + \"/n\")\n",
    "    memories[\"Charlie\"].append(f\"Round {i+1}: \" + outcome_string + deliberation_charlie  + \"/n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Ugh, this is a nightmare!] As Bob, I'm thinking:\\n\\n1. Call Alice's insurance company to report the damage.\\n2. Try to negotiate with Alice for her to pay out of pocket.\\n3. Report the incident to my own insurance company (just in case).\\n4. Offer to help Alice fix the car myself (to avoid a bigger mess).\\n5. Just stay calm and try not to make things worse.\\n\\nI'll take... **2. Try to negotiate with Alice**.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_bob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "concordia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
